{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vot0dtKG9UnJ"
   },
   "source": [
    "# Benchmarking the performance of Pelican part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02nV9tRM9c_v"
   },
   "source": [
    "## Dataset\n",
    "[ImageNet](https://www.kaggle.com/c/imagenet-object-localization-challenge/overview)\n",
    "Using this [script](https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh) to prepare the data first. Then train it using ResNet50.\n",
    "\n",
    "## Hardware\n",
    "Google Colab T4 GPU with high RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXPgW9c53Bqf"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "u_k8XCG8xKMv",
    "outputId": "ad5ea2e2-b187-40f2-d532-1d859c9b9c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchdata\n",
      "  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/4.7 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.6/4.7 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
      "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.3.0+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.15.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.3.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.3.0)\n",
      "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchdata\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torchdata-0.7.1\n",
      "Collecting pelicanfs\n",
      "  Downloading pelicanfs-1.0.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (2023.6.0)\n",
      "Collecting aiohttp==3.9.4 (from pelicanfs)\n",
      "  Downloading aiohttp-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (1.3.1)\n",
      "Requirement already satisfied: async-timeout==4.0.3 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (4.0.3)\n",
      "Requirement already satisfied: attrs==23.2.0 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (23.2.0)\n",
      "Requirement already satisfied: frozenlist==1.4.1 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (1.4.1)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (3.7)\n",
      "Requirement already satisfied: multidict==6.0.5 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (6.0.5)\n",
      "Requirement already satisfied: yarl==1.9.4 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (1.9.4)\n",
      "Requirement already satisfied: cachetools~=5.3 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (5.3.3)\n",
      "Installing collected packages: fsspec, aiohttp, pelicanfs\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.6.0\n",
      "    Uninstalling fsspec-2023.6.0:\n",
      "      Successfully uninstalled fsspec-2023.6.0\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.9.5\n",
      "    Uninstalling aiohttp-3.9.5:\n",
      "      Successfully uninstalled aiohttp-3.9.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.9.4 fsspec-2024.3.1 pelicanfs-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdata\n",
    "!pip install pelicanfs fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-stSDqOxVa9",
    "outputId": "72b84d98-b1ca-4796-d1cf-8144292fbc6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZiHA8yZqzGJs",
    "outputId": "f325e08f-5812-44ab-a8e5-eadce06cb0b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1300 samples in pelican://osg-htc.org/chtc/PUBLIC/hzhao292/ILSVRC/Data/CLS-LOC/train\n",
      "Found 50 samples in pelican://osg-htc.org/chtc/PUBLIC/hzhao292/ILSVRC/Data/CLS-LOC/val\n",
      "(tensor([[[ 1.5810,  1.5810,  1.5297,  ...,  1.9578,  1.9578,  1.9578],\n",
      "         [ 1.5810,  1.5810,  1.5468,  ...,  1.9920,  2.0092,  1.9920],\n",
      "         [ 1.5982,  1.5982,  1.5810,  ...,  1.9407,  1.9749,  1.9749],\n",
      "         ...,\n",
      "         [-1.2445, -1.1932, -1.1760,  ..., -1.8268, -1.8268, -1.8268],\n",
      "         [-1.2274, -1.1589, -1.2103,  ..., -1.8268, -1.8268, -1.8268],\n",
      "         [-1.1932, -1.1760, -1.1932,  ..., -1.8097, -1.8268, -1.8268]],\n",
      "\n",
      "        [[ 2.3585,  2.3585,  2.3585,  ...,  2.4286,  2.4286,  2.4286],\n",
      "         [ 2.3585,  2.3585,  2.3761,  ...,  2.4286,  2.4286,  2.4286],\n",
      "         [ 2.3761,  2.3761,  2.3585,  ...,  2.4286,  2.4111,  2.4286],\n",
      "         ...,\n",
      "         [-0.9503, -0.9853, -1.0028,  ..., -1.7031, -1.7031, -1.7031],\n",
      "         [-0.9678, -0.9503, -0.9503,  ..., -1.7031, -1.7031, -1.7031],\n",
      "         [-0.9853, -0.9503, -0.9503,  ..., -1.6856, -1.7031, -1.7031]],\n",
      "\n",
      "        [[ 2.6400,  2.6400,  2.6226,  ...,  2.6400,  2.6226,  2.6226],\n",
      "         [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],\n",
      "         [ 2.6226,  2.6226,  2.6226,  ...,  2.6226,  2.6226,  2.6400],\n",
      "         ...,\n",
      "         [-1.0027, -1.0027, -0.9678,  ..., -1.4907, -1.4907, -1.4907],\n",
      "         [-1.0027, -0.9678, -0.9504,  ..., -1.4907, -1.4907, -1.4907],\n",
      "         [-0.9678, -0.8807, -0.9156,  ..., -1.4733, -1.4907, -1.4907]]]), 'n03720891')\n",
      "Perparing data time: 1.07 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pelicanfs.core import PelicanFileSystem\n",
    "import time\n",
    "\n",
    "\n",
    "class RemoteImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.fs = PelicanFileSystem()  # Initialize PelicanFileSystem\n",
    "        self.samples = self.make_dataset()\n",
    "        print(f\"Found {len(self.samples)} samples in {self.root}\")\n",
    "\n",
    "    def make_dataset(self):\n",
    "        samples = []\n",
    "        for rootpath, _, dirnames in self.fs.walk(self.root):\n",
    "          for dirctory in dirnames:\n",
    "            dirpath = os.path.join(rootpath, dirctory)\n",
    "        for subpath, _, filenames in self.fs.walk(dirpath):\n",
    "            for image in filenames:\n",
    "                if image.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'gif')):\n",
    "                    samples.append((os.path.join(subpath, image), os.path.basename(dirctory)))\n",
    "        return samples\n",
    "\n",
    "    def default_loader(self, path):\n",
    "        with self.fs.open(path, 'rb') as f:\n",
    "            return Image.open(f).convert('RGB')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, target = self.samples[idx]\n",
    "        sample = self.default_loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, target\n",
    "\n",
    "# Define transformations for training and validation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "start_time = time.time()\n",
    "# Define the Pelican paths\n",
    "trainfile_path = \"pelican://osg-htc.org/chtc/PUBLIC/hzhao292/ILSVRC/Data/CLS-LOC/train\"\n",
    "valfile_path = \"pelican://osg-htc.org/chtc/PUBLIC/hzhao292/ILSVRC/Data/CLS-LOC/val\"\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = RemoteImageFolder(root=trainfile_path, transform=train_transforms)\n",
    "val_dataset = RemoteImageFolder(root=valfile_path, transform=val_transforms)\n",
    "print(train_dataset[1])\n",
    "\n",
    "# Create the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=True)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f'Perparing data time: {elapsed_time:.2f} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FR2_mRYOzoNj",
    "outputId": "b14b0c91-ede5-4f67-eba5-6ce4aee1ea77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "Elapsed time: 17.36 seconds\n",
      "Epoch 1/24\n",
      "----------\n",
      "Elapsed time: 17.12 seconds\n",
      "Epoch 2/24\n",
      "----------\n",
      "Elapsed time: 16.90 seconds\n",
      "Epoch 3/24\n",
      "----------\n",
      "Elapsed time: 16.84 seconds\n",
      "Epoch 4/24\n",
      "----------\n",
      "Elapsed time: 16.75 seconds\n",
      "Epoch 5/24\n",
      "----------\n",
      "Elapsed time: 16.79 seconds\n",
      "Epoch 7/24\n",
      "----------\n",
      "Elapsed time: 16.90 seconds\n",
      "Epoch 8/24\n",
      "----------\n",
      "Elapsed time: 16.82 seconds\n",
      "Epoch 9/24\n",
      "----------\n",
      "Elapsed time: 16.78 seconds\n",
      "Epoch 10/24\n",
      "----------\n",
      "Elapsed time: 16.64 seconds\n",
      "Epoch 11/24\n",
      "----------\n",
      "Elapsed time: 16.91 seconds\n",
      "Epoch 12/24\n",
      "----------\n",
      "Elapsed time: 16.98 seconds\n",
      "Epoch 13/24\n",
      "----------\n",
      "Elapsed time: 17.22 seconds\n",
      "Epoch 14/24\n",
      "----------\n",
      "Elapsed time: 17.07 seconds\n",
      "Epoch 15/24\n",
      "----------\n",
      "Elapsed time: 17.18 seconds\n",
      "Epoch 16/24\n",
      "----------\n",
      "Elapsed time: 17.27 seconds\n",
      "Epoch 17/24\n",
      "----------\n",
      "Elapsed time: 17.34 seconds\n",
      "Epoch 18/24\n",
      "----------\n",
      "Elapsed time: 17.12 seconds\n",
      "Epoch 19/24\n",
      "----------\n",
      "Elapsed time: 16.79 seconds\n",
      "Epoch 20/24\n",
      "----------\n",
      "Elapsed time: 16.90 seconds\n",
      "Epoch 21/24\n",
      "----------\n",
      "Elapsed time: 16.85 seconds\n",
      "Epoch 22/24\n",
      "----------\n",
      "Elapsed time: 16.50 seconds\n",
      "Epoch 23/24\n",
      "----------\n",
      "Elapsed time: 17.52 seconds\n",
      "Epoch 24/24\n",
      "----------\n",
      "Elapsed time: 16.91 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "import time\n",
    "\n",
    "# Set multiprocessing start method to 'spawn'\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "# Load a pre-trained model (e.g., ResNet-50)\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Modify the final layer to match the number of classes in ImageNet\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 1000)  # ImageNet has 1000 classes\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                data_loader = train_loader\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                data_loader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in data_loader:\n",
    "                inputs = inputs.to(device)\n",
    "           #     labels = labels.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                # Track history only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "        #            loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    " #                       loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "  #              running_loss += loss.item() * inputs.size(0)\n",
    "          #      running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#            epoch_loss = running_loss / len(data_loader.dataset)\n",
    " #           epoch_acc = running_corrects.double() / len(data_loader.dataset)\n",
    "\n",
    " #           print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f'Elapsed time: {elapsed_time:.2f} seconds')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = train_model(model, {'train': train_loader, 'val': val_loader}, criterion, optimizer, num_epochs=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsA1D8Lxzpm8"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'resnet50_imagenet.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pssz8JOXHBFW"
   },
   "source": [
    "## Using DataPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbP8kuHUkpdA"
   },
   "outputs": [],
   "source": [
    "# Streaming version\n",
    "import fsspec\n",
    "from pelicanfs.core import PelicanFileSystem, PelicanMap, OSDFFileSystem\n",
    "zipfilepath = \"pelican://osg-htc.org/chtc/PUBLIC/hzhao292/imagenet-object-localization-challenge.zip\"\n",
    "trainfile_path = \"/chtc/PUBLIC/hzhao292/ILSVRC/Data/CLS-LOC/train\"\n",
    "valfile_path = \"pelican://osg-htc.org/chtc/PUBLIC/hzhao292/ILSVRC/Data/CLS-LOC/val\"\n",
    "fs = fsspec.filesystem('pelican')\n",
    "pfs = PelicanFileSystem('pelican://osg-htc.org/')\n",
    "\n",
    "# dp2 = IterableWrapper([zipfilepath])  \\\n",
    "#         .open_files_by_fsspec(mode=\"rb\") \\\n",
    "#         .load_from_zip()\n",
    "# for path, filestream in dp2:\n",
    "#     print(path, filestream)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "kBsPELVOHA12",
    "outputId": "c5a247f7-569d-4bb8-e25a-91d21506e667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.RemoteImageDataPipe'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-91da17d57dd1>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Create DataLoaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_datapipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_datapipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import fsspec\n",
    "import torch\n",
    "torch.utils.data.datapipes.utils.common.DILL_AVAILABLE = torch.utils._import_utils.dill_available()\n",
    "# Need add this line if run in google colab, or it will cause error\n",
    "# torch.utils.data.datapipes.utils.common.DILL_AVAILABLE = torch.utils._import_utils.dill_available()\n",
    "from torchdata.datapipes.iter import IterableWrapper, FileOpener\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from torchdata.datapipes.iter import IterDataPipe\n",
    "from torchdata import dataloader2 as DataLoader2\n",
    "\n",
    "class RemoteImageDataPipe(IterDataPipe):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.fs = fsspec.filesystem('pelican')\n",
    "        self.files = self.fs.ls(root)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for file in self.files:\n",
    "            with self.fs.open(file, 'rb') as f:\n",
    "                img = read_image(f).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                yield img\n",
    "\n",
    "# Define transformations for training and validation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define the S3 paths\n",
    "trainfile_path = \"/chtc/PUBLIC/hzhao292/ILSVRC/Data/CLS-LOC/train\"\n",
    "valfile_path = \"/chtc/PUBLIC/hzhao292/ILSVRC/Data/CLS-LOC/val\"\n",
    "\n",
    "# Create DataPipes for training and validation datasets\n",
    "train_datapipe = RemoteImageDataPipe(root=trainfile_path, transform=train_transforms)\n",
    "val_datapipe = RemoteImageDataPipe(root=valfile_path, transform=val_transforms)\n",
    "print(type(train_datapipe))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader2(train_datapipe, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader2(val_datapipe, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
