{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vot0dtKG9UnJ"
      },
      "source": [
        "# Benchmarking the performance of Pelican part2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02nV9tRM9c_v"
      },
      "source": [
        "## Dataset\n",
        "[ImageNet](https://www.kaggle.com/c/imagenet-object-localization-challenge/overview)\n",
        "Using this [script](https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh) to prepare the data first. Then train it using ResNet50.\n",
        "\n",
        "## Hardware\n",
        "Google Colab T4 GPU with high RAM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dXPgW9c53Bqf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_k8XCG8xKMv",
        "outputId": "9c117676-fb90-4f88-ac7b-3bf5e57c9190",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdata\n",
            "  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchdata\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torchdata-0.7.1\n",
            "Collecting pelicanfs\n",
            "  Downloading pelicanfs-1.0.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (2023.6.0)\n",
            "Collecting aiohttp==3.9.4 (from pelicanfs)\n",
            "  Downloading aiohttp-3.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal==1.3.1 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (1.3.1)\n",
            "Requirement already satisfied: async-timeout==4.0.3 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (4.0.3)\n",
            "Requirement already satisfied: attrs==23.2.0 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (23.2.0)\n",
            "Requirement already satisfied: frozenlist==1.4.1 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (1.4.1)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna==3.7 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (3.7)\n",
            "Requirement already satisfied: multidict==6.0.5 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (6.0.5)\n",
            "Requirement already satisfied: yarl==1.9.4 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (1.9.4)\n",
            "Requirement already satisfied: cachetools~=5.3 in /usr/local/lib/python3.10/dist-packages (from pelicanfs) (5.3.3)\n",
            "Installing collected packages: fsspec, aiohttp, pelicanfs\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.6.0\n",
            "    Uninstalling fsspec-2023.6.0:\n",
            "      Successfully uninstalled fsspec-2023.6.0\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.9.5\n",
            "    Uninstalling aiohttp-3.9.5:\n",
            "      Successfully uninstalled aiohttp-3.9.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiohttp-3.9.4 fsspec-2024.3.1 pelicanfs-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdata\n",
        "!pip install pelicanfs fsspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-stSDqOxVa9",
        "outputId": "d518d021-9732-4cd2-9baf-9f67b3d9fcb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiHA8yZqzGJs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, optim\n",
        "import fsspec\n",
        "from pelicanfs.core import PelicanFileSystem\n",
        "import time\n",
        "from PIL import Image\n",
        "from fsspec.implementations.local import LocalFileSystem\n",
        "from torchvision.datasets import VisionDataset\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class RemoteImageFolder(VisionDataset):\n",
        "\n",
        "    def __init__(self, root,transform=None, target_transform=None):\n",
        "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
        "        if os.path.isdir(root):\n",
        "            self._init_local(root)\n",
        "        else:\n",
        "            self._init_remote(root)\n",
        "\n",
        "    def _init_local(self, root):\n",
        "        print(f\"Initializing local dataset from {root}\")\n",
        "        self.root = root\n",
        "        self.fs = LocalFileSystem()\n",
        "        self.classes = sorted(os.listdir(root))\n",
        "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
        "        self.imgs = self._make_dataset_local()\n",
        "\n",
        "    def _init_remote(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.fs = PelicanFileSystem(\"pelican://osg-htc.org\")\n",
        "\n",
        "        self.classes = sorted([item['name'] for item in self.fs.ls(root)])\n",
        "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
        "\n",
        "        self.imgs = self._make_dataset_remote()\n",
        "\n",
        "    def _make_dataset_local(self):\n",
        "        images = []\n",
        "        for class_idx, cls_name in enumerate(self.classes):\n",
        "            class_path = os.path.join(self.root, cls_name)\n",
        "            if not os.path.isdir(class_path):\n",
        "                continue\n",
        "            for img_name in os.listdir(class_path):\n",
        "                img_name = img_name.lower()\n",
        "                if img_name.endswith('.jpg') or img_name.endswith('.jpeg') or img_name.endswith('.png'):\n",
        "                    img_path = os.path.join(class_path, img_name)\n",
        "                    images.append((img_path, class_idx))\n",
        "        return images\n",
        "\n",
        "    def _make_dataset_remote(self):\n",
        "        images = []\n",
        "        for class_idx, cls_name in enumerate(self.classes):\n",
        "            class_path = os.path.join(self.root, cls_name)\n",
        "            files = self.fs.ls(class_path)\n",
        "            for item in files:\n",
        "                img_path = item['name']\n",
        "                if img_path.lower().endswith('.jpg') or img_path.lower().endswith('.jpeg') or img_path.lower().endswith('.png'):\n",
        "                    images.append((img_path, class_idx))\n",
        "        print(\"len(images): \", len(images))\n",
        "        return images\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path, target = self.imgs[index]\n",
        "        if isinstance(self.fs, PelicanFileSystem):\n",
        "            with self.fs.open(img_path, 'rb') as f:\n",
        "                img = Image.open(f).convert('RGB')\n",
        "        else:\n",
        "            img = read_image(img_path)\n",
        "            img = transforms.ToPILImage()(img)\n",
        "\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR2_mRYOzoNj",
        "outputId": "fc9c0373-c92d-409d-b508-8cdcf3e73fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(images):  9469\n",
            "len(images):  3925\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import torch.multiprocessing as mp\n",
        "import time\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),  # Ensure ToTensor is included\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),  # Ensure ToTensor is included\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Define the Pelican paths\n",
        "trainfile_path = \"/chtc/PUBLIC/hzhao292/ILSVRC/Data/CLS-LOC/train\"\n",
        "valfile_path = \"/chtc/PUBLIC/hzhao292/ILSVRC/Data/CLS-LOC/val\"\n",
        "\n",
        "dev_trainfile_path = \"/chtc/PUBLIC/hzhao292/ImageNetMini/train\"\n",
        "dev_valfile_path = \"/chtc/PUBLIC/hzhao292/ImageNetMini/val\"\n",
        "\n",
        "# Load the datasets\n",
        "train_dataset = RemoteImageFolder(root=dev_trainfile_path, transform=train_transforms)\n",
        "val_dataset = RemoteImageFolder(root=dev_valfile_path, transform=val_transforms)\n",
        "\n",
        "# Create the dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(train_dataset.classes))\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training and validation loop\n",
        "num_epochs = 10\n",
        "\n",
        "print(\"Training started.\")\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch\", epoch+1, \"started.\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    i=0\n",
        "    for inputs, labels in train_loader:\n",
        "        i+=1\n",
        "        if i%10==0:\n",
        "            print(\"Batch \", i)\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader.dataset)\n",
        "    accuracy = correct / total\n",
        "    end_time = time.time()\n",
        "    time_taken = end_time - start_time\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}, Time Taken: {time_taken:.2f} seconds\")\n",
        "\n",
        "print(\"Training completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAROVPOIsyNh",
        "outputId": "ffd8e6ef-919e-4260-e86a-b0f93a6452e4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 51.7MB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started.\n",
            "Epoch 1 started.\n",
            "Batch  10\n",
            "Batch  20\n",
            "Batch  30\n",
            "Batch  40\n",
            "Batch  50\n",
            "Batch  60\n",
            "Batch  70\n",
            "Batch  80\n",
            "Batch  90\n",
            "Batch  100\n",
            "Batch  110\n",
            "Batch  120\n",
            "Batch  130\n",
            "Batch  140\n",
            "Batch  150\n",
            "Batch  160\n",
            "Batch  170\n",
            "Batch  180\n",
            "Batch  190\n",
            "Batch  200\n",
            "Batch  210\n",
            "Batch  220\n",
            "Batch  230\n",
            "Batch  240\n",
            "Batch  250\n",
            "Batch  260\n",
            "Batch  270\n",
            "Batch  280\n",
            "Batch  290\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Training Loss: 1.1407\n",
            "Epoch 1/10, Validation Loss: 0.6926, Accuracy: 0.7809, Time Taken: 1138.80 seconds\n",
            "Epoch 2 started.\n",
            "Batch  10\n",
            "Batch  20\n",
            "Batch  30\n",
            "Batch  40\n",
            "Batch  50\n",
            "Batch  60\n",
            "Batch  70\n",
            "Batch  80\n",
            "Batch  90\n",
            "Batch  100\n",
            "Batch  110\n",
            "Batch  120\n",
            "Batch  130\n",
            "Batch  140\n",
            "Batch  150\n",
            "Batch  160\n",
            "Batch  170\n",
            "Batch  180\n",
            "Batch  190\n",
            "Batch  200\n",
            "Batch  210\n",
            "Batch  220\n",
            "Batch  230\n",
            "Batch  240\n",
            "Batch  250\n",
            "Batch  260\n",
            "Batch  270\n",
            "Batch  280\n",
            "Batch  290\n",
            "Epoch 2/10, Training Loss: 0.8152\n",
            "Epoch 2/10, Validation Loss: 0.6254, Accuracy: 0.8020, Time Taken: 1113.87 seconds\n",
            "Epoch 3 started.\n",
            "Batch  10\n",
            "Batch  20\n",
            "Batch  30\n",
            "Batch  40\n",
            "Batch  50\n",
            "Batch  60\n",
            "Batch  70\n",
            "Batch  80\n",
            "Batch  90\n",
            "Batch  100\n",
            "Batch  110\n",
            "Batch  120\n",
            "Batch  130\n",
            "Batch  140\n",
            "Batch  150\n",
            "Batch  160\n",
            "Batch  170\n",
            "Batch  180\n",
            "Batch  190\n",
            "Batch  200\n",
            "Batch  210\n",
            "Batch  220\n",
            "Batch  230\n",
            "Batch  240\n",
            "Batch  250\n",
            "Batch  260\n",
            "Batch  270\n",
            "Batch  280\n",
            "Batch  290\n",
            "Epoch 3/10, Training Loss: 0.7164\n",
            "Epoch 3/10, Validation Loss: 0.4831, Accuracy: 0.8522, Time Taken: 1074.37 seconds\n",
            "Epoch 4 started.\n",
            "Batch  10\n",
            "Batch  20\n",
            "Batch  30\n",
            "Batch  40\n",
            "Batch  50\n",
            "Batch  60\n",
            "Batch  70\n",
            "Batch  80\n",
            "Batch  90\n",
            "Batch  100\n",
            "Batch  110\n",
            "Batch  120\n",
            "Batch  130\n",
            "Batch  140\n",
            "Batch  150\n",
            "Batch  160\n",
            "Batch  170\n",
            "Batch  180\n",
            "Batch  190\n",
            "Batch  200\n",
            "Batch  210\n",
            "Batch  220\n",
            "Batch  230\n",
            "Batch  240\n",
            "Batch  250\n",
            "Batch  260\n",
            "Batch  270\n",
            "Batch  280\n",
            "Batch  290\n",
            "Epoch 4/10, Training Loss: 0.6283\n",
            "Epoch 4/10, Validation Loss: 0.4641, Accuracy: 0.8548, Time Taken: 1094.85 seconds\n",
            "Epoch 5 started.\n",
            "Batch  10\n",
            "Batch  20\n",
            "Batch  30\n",
            "Batch  40\n",
            "Batch  50\n",
            "Batch  60\n",
            "Batch  70\n",
            "Batch  80\n",
            "Batch  90\n",
            "Batch  100\n",
            "Batch  110\n",
            "Batch  120\n",
            "Batch  130\n",
            "Batch  140\n",
            "Batch  150\n",
            "Batch  160\n",
            "Batch  170\n",
            "Batch  180\n",
            "Batch  190\n",
            "Batch  200\n",
            "Batch  210\n",
            "Batch  220\n",
            "Batch  230\n",
            "Batch  240\n",
            "Batch  250\n",
            "Batch  260\n",
            "Batch  270\n",
            "Batch  280\n",
            "Batch  290\n",
            "Epoch 5/10, Training Loss: 0.6057\n",
            "Epoch 5/10, Validation Loss: 0.4219, Accuracy: 0.8683, Time Taken: 1086.80 seconds\n",
            "Epoch 6 started.\n",
            "Batch  10\n",
            "Batch  20\n",
            "Batch  30\n",
            "Batch  40\n",
            "Batch  50\n",
            "Batch  60\n",
            "Batch  70\n",
            "Batch  80\n",
            "Batch  90\n",
            "Batch  100\n",
            "Batch  110\n",
            "Batch  120\n",
            "Batch  130\n",
            "Batch  140\n",
            "Batch  150\n",
            "Batch  160\n",
            "Batch  170\n",
            "Batch  180\n",
            "Batch  190\n",
            "Batch  200\n",
            "Batch  210\n",
            "Batch  220\n",
            "Batch  230\n",
            "Batch  240\n",
            "Batch  250\n",
            "Batch  260\n",
            "Batch  270\n",
            "Batch  280\n",
            "Batch  290\n",
            "Epoch 6/10, Training Loss: 0.5727\n",
            "Epoch 6/10, Validation Loss: 0.3740, Accuracy: 0.8792, Time Taken: 1094.54 seconds\n",
            "Epoch 7 started.\n",
            "Batch  10\n",
            "Batch  20\n",
            "Batch  30\n",
            "Batch  40\n",
            "Batch  50\n",
            "Batch  60\n",
            "Batch  70\n",
            "Batch  80\n",
            "Batch  90\n",
            "Batch  100\n",
            "Batch  110\n",
            "Batch  120\n",
            "Batch  130\n",
            "Batch  140\n",
            "Batch  150\n",
            "Batch  160\n",
            "Batch  170\n",
            "Batch  180\n",
            "Batch  190\n",
            "Batch  200\n",
            "Batch  210\n",
            "Batch  220\n",
            "Batch  230\n",
            "Batch  240\n",
            "Batch  250\n",
            "Batch  260\n",
            "Batch  270\n",
            "Batch  280\n",
            "Batch  290\n",
            "Epoch 7/10, Training Loss: 0.5550\n",
            "Epoch 7/10, Validation Loss: 0.4256, Accuracy: 0.8591, Time Taken: 1119.60 seconds\n",
            "Epoch 8 started.\n",
            "Batch  10\n",
            "Batch  20\n",
            "Batch  30\n",
            "Batch  40\n",
            "Batch  50\n",
            "Batch  60\n",
            "Batch  70\n",
            "Batch  80\n",
            "Batch  90\n",
            "Batch  100\n",
            "Batch  110\n",
            "Batch  120\n",
            "Batch  130\n",
            "Batch  140\n",
            "Batch  150\n",
            "Batch  160\n",
            "Batch  170\n",
            "Batch  180\n",
            "Batch  190\n",
            "Batch  200\n",
            "Batch  210\n",
            "Batch  220\n",
            "Batch  230\n",
            "Batch  240\n",
            "Batch  250\n",
            "Batch  260\n",
            "Batch  270\n",
            "Batch  280\n",
            "Batch  290\n",
            "Epoch 8/10, Training Loss: 0.5281\n",
            "Epoch 8/10, Validation Loss: 0.3873, Accuracy: 0.8785, Time Taken: 1090.34 seconds\n",
            "Epoch 9 started.\n",
            "Batch  10\n",
            "Batch  20\n",
            "Batch  30\n",
            "Batch  40\n",
            "Batch  50\n",
            "Batch  60\n",
            "Batch  70\n",
            "Batch  80\n",
            "Batch  90\n",
            "Batch  100\n",
            "Batch  110\n",
            "Batch  120\n",
            "Batch  130\n",
            "Batch  140\n",
            "Batch  150\n",
            "Batch  160\n",
            "Batch  170\n",
            "Batch  180\n",
            "Batch  190\n",
            "Batch  200\n",
            "Batch  210\n",
            "Batch  220\n",
            "Batch  230\n",
            "Batch  240\n",
            "Batch  250\n",
            "Batch  260\n",
            "Batch  270\n",
            "Batch  280\n",
            "Batch  290\n",
            "Epoch 9/10, Training Loss: 0.5102\n",
            "Epoch 9/10, Validation Loss: 0.3713, Accuracy: 0.8897, Time Taken: 1142.84 seconds\n",
            "Epoch 10 started.\n",
            "Batch  10\n",
            "Batch  20\n",
            "Batch  30\n",
            "Batch  40\n",
            "Batch  50\n",
            "Batch  60\n",
            "Batch  70\n",
            "Batch  80\n",
            "Batch  90\n",
            "Batch  100\n",
            "Batch  110\n",
            "Batch  120\n",
            "Batch  130\n",
            "Batch  140\n",
            "Batch  150\n",
            "Batch  160\n",
            "Batch  170\n",
            "Batch  180\n",
            "Batch  190\n",
            "Batch  200\n",
            "Batch  210\n",
            "Batch  220\n",
            "Batch  230\n",
            "Batch  240\n",
            "Batch  250\n",
            "Batch  260\n",
            "Batch  270\n",
            "Batch  280\n",
            "Batch  290\n",
            "Epoch 10/10, Training Loss: 0.4883\n",
            "Epoch 10/10, Validation Loss: 0.3427, Accuracy: 0.8917, Time Taken: 1141.83 seconds\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsA1D8Lxzpm8"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'resnet50_imagenet.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pssz8JOXHBFW"
      },
      "source": [
        "## Using DataPipe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Streaming version\n",
        "import fsspec\n",
        "from pelicanfs.core import PelicanFileSystem, PelicanMap, OSDFFileSystem\n",
        "zipfilepath = \"pelican://osg-htc.org/chtc/PUBLIC/hzhao292/imagenet-object-localization-challenge.zip\"\n",
        "trainfile_path = \"/chtc/PUBLIC/hzhao292/ILSVRC/Data/CLS-LOC/train\"\n",
        "valfile_path = \"pelican://osg-htc.org/chtc/PUBLIC/hzhao292/ILSVRC/Data/CLS-LOC/val\"\n",
        "fs = fsspec.filesystem('pelican')\n",
        "pfs = PelicanFileSystem('pelican://osg-htc.org/')\n",
        "\n",
        "# dp2 = IterableWrapper([zipfilepath])  \\\n",
        "#         .open_files_by_fsspec(mode=\"rb\") \\\n",
        "#         .load_from_zip()\n",
        "# for path, filestream in dp2:\n",
        "#     print(path, filestream)\n",
        "#     break"
      ],
      "metadata": {
        "id": "FbP8kuHUkpdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "kBsPELVOHA12",
        "outputId": "c5a247f7-569d-4bb8-e25a-91d21506e667"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.RemoteImageDataPipe'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'module' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-91da17d57dd1>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Create DataLoaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_datapipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_datapipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ],
      "source": [
        "import fsspec\n",
        "import torch\n",
        "torch.utils.data.datapipes.utils.common.DILL_AVAILABLE = torch.utils._import_utils.dill_available()\n",
        "# Need add this line if run in google colab, or it will cause error\n",
        "# torch.utils.data.datapipes.utils.common.DILL_AVAILABLE = torch.utils._import_utils.dill_available()\n",
        "from torchdata.datapipes.iter import IterableWrapper, FileOpener\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import read_image\n",
        "from torchdata.datapipes.iter import IterDataPipe\n",
        "from torchdata import dataloader2 as DataLoader2\n",
        "\n",
        "class RemoteImageDataPipe(IterDataPipe):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.fs = fsspec.filesystem('pelican')\n",
        "        self.files = self.fs.ls(root)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for file in self.files:\n",
        "            with self.fs.open(file, 'rb') as f:\n",
        "                img = read_image(f).convert('RGB')\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                yield img\n",
        "\n",
        "# Define transformations for training and validation\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "/Users/a/Documents/2024summerintern/IntegratePelicanwithPytorch/benchmark/Benchmark2.ipynb\n",
        "# Define the S3 paths\n",
        "trainfile_path = \"/chtc/PUBLIC/hzhao292/ILSVRC/Data/CLS-LOC/train\"\n",
        "valfile_path = \"/chtc/PUBLIC/hzhao292/ILSVRC/Data/CLS-LOC/val\"\n",
        "\n",
        "# Create DataPipes for training and validation datasets\n",
        "train_datapipe = RemoteImageDataPipe(root=trainfile_path, transform=train_transforms)\n",
        "val_datapipe = RemoteImageDataPipe(root=valfile_path, transform=val_transforms)\n",
        "print(type(train_datapipe))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader2(train_datapipe, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader2(val_datapipe, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "for i in train_loader:\n",
        "    print(i)\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}